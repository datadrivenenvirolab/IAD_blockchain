{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing in all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, re, json, string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from requests.packages.urllib3.exceptions import *\n",
    "from socket import gaierror\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The stem url for api calls. manipulating this variable\n",
    "could allow for calls referencing pages written in languages other than english.\n",
    "\"\"\"\n",
    "API_URL = 'http://en.wikipedia.org/w/api.php'\n",
    "\n",
    "def get_page(title):\n",
    "    if title is not None:\n",
    "        # retry page processing as sometimes network errors\n",
    "        # can occur. Most of the time an exception will not be thrown.\n",
    "        try:\n",
    "            page = requests.get('https://en.wikipedia.org/wiki/' + title)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                page = requests.get('https://en.wikipedia.org/wiki/' + title)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                return None\n",
    "        return BeautifulSoup(page.content, 'lxml')\n",
    "    \n",
    "def get_pages(titles, num_workers=20):\n",
    "    \"\"\"\n",
    "    Use a thread pool to download multiple pages for faster batch processing.\n",
    "    Can't use the standard wikimedia api multi-title feature as it increases\n",
    "    the likelihood useful data won't be returned.\n",
    "    \"\"\"\n",
    "    if type(titles) == list and len(titles) > 0:\n",
    "        if len(titles) < num_workers: num_workers = len(titles)\n",
    "        pool = ThreadPool(num_workers)\n",
    "        pages = pool.map(get_page, titles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return [i for i in pages if i]\n",
    "    \n",
    "def get_url(url):\n",
    "    if url is not None:\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                page = requests.get(url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                return None\n",
    "        return BeautifulSoup(page.content, 'lxml')\n",
    "    \n",
    "def get_urls(urls, num_workers=20):\n",
    "    \"\"\"\n",
    "    Use a thread pool to download multiple urls for faster batch processing.\n",
    "    \"\"\"\n",
    "    if type(urls) == list and len(urls) > 0:\n",
    "        if len(urls) < num_workers: num_workers = len(urls)\n",
    "        pool = ThreadPool(num_workers)\n",
    "        pages = pool.map(get_url, urls)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return [i for i in pages if i]\n",
    "    \n",
    "def search(name):\n",
    "    \"\"\"\n",
    "    queries the wikimedia API to return {limit} matches to the search term\n",
    "    \"\"\"\n",
    "    titles = _search_query(name)\n",
    "    return titles\n",
    "    \n",
    "def _search_query(name):\n",
    "    \"\"\"\n",
    "    the underlying api query that powers the search function.\n",
    "    \"\"\"\n",
    "    url = 'https://en.wikipedia.org/w/index.php'\n",
    "    \n",
    "    params = {\n",
    "        'sort': 'relevance',\n",
    "        'search': name,\n",
    "        'profile': 'advanced',\n",
    "        'fulltext': 1\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    if r.status_code == 200:\n",
    "        page = BeautifulSoup(r.content, 'lxml')\n",
    "        titles = page.find_all('div', {'class': 'mw-search-result-heading'})\n",
    "        titles = [t.find('a')['title'] for t in titles]\n",
    "        return titles\n",
    "    else:\n",
    "        time.sleep(0.5)\n",
    "        r = requests.get(API_URL, params=params)\n",
    "        return titles\n",
    "\n",
    "\n",
    "# # Get Country Data\n",
    "# iso, wiki title mapping\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "iso_page = requests.get('https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3').content\n",
    "iso_page = BeautifulSoup(iso_page, 'lxml')\n",
    "iso_page = iso_page.find('div', {'class': 'plainlist'})\n",
    "\n",
    "iso_title = {}\n",
    "title_iso = {}\n",
    "\n",
    "for i in iso_page.find_all('li'):\n",
    "    tmp = {}\n",
    "    iso = i.find('span').text\n",
    "    title = i.find('a')['title']\n",
    "    iso_title[iso] = title\n",
    "    title_iso[title] = iso\n",
    "    \n",
    "# Adding in missing ISOs on wiki\n",
    "iso_title['XKX'] = 'Kosovo'\n",
    "title_iso['Kosovo'] = 'XKX'\n",
    "    \n",
    "# # Process Infobox\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "def clean(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\xa0', ' ')\n",
    "    x = re.sub(r'\\[.*\\]', '', x)\n",
    "    x = re.sub(r'\\(.*\\)', '', x)\n",
    "    x = re.sub('\\.[^\\s\\d]+?(\\s|$)', '', x)\n",
    "    x = x.replace('- ', '').replace('•', '').replace(' \\n', ', ').replace('\\n', ', ').strip()\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    return x\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def fix_newlines(infobox):\n",
    "    # add some newlines to make lists look nicer.\n",
    "    for br in infobox.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "    for img in infobox.find_all(\"img\"):\n",
    "        img.replace_with(\"\")\n",
    "    for img in infobox.find_all('a', {'class': 'image'}):\n",
    "        img.replace_with(\"\")\n",
    "    for li in infobox.find_all(\"li\"):\n",
    "        li.replace_with(li.text + '\\n')\n",
    "        \n",
    "def get_top_region(infobox, data):\n",
    "    # Australian cities store region at the top.\n",
    "    if infobox.find('span', {'class': 'region'}):\n",
    "        data['region_aus'] = [infobox.find('span', {'class': 'region'}).text]\n",
    "        data['region_aus_links'] = [infobox.find('span', {'class': 'region'}).find('a')]\n",
    "        data['region_aus_links'] = [a['href'] for a in data['region_aus_links'] if a and a.has_attr('href')]\n",
    "    return data\n",
    "\n",
    "def get_cats(infobox, data):\n",
    "    cat = infobox.find('div', {'class': 'category'})\n",
    "    if cat:\n",
    "        cat = cat.text\n",
    "        data['category_label'] = cat\n",
    "    return data\n",
    "\n",
    "def get_infobox(page):\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    infobox = page.find('table', {'class': 'infobox'})\n",
    "    if not infobox:\n",
    "        return {}\n",
    "    \n",
    "    fix_newlines(infobox)\n",
    "    data = get_top_region(infobox, data)\n",
    "    data = get_cats(infobox, data)\n",
    "\n",
    "    section = ''\n",
    "    section_year = None\n",
    "    sect_change = False\n",
    "    \n",
    "    # iterate through each table row.\n",
    "    for i in infobox.find_all('tr'):\n",
    "        key = i.find('th')\n",
    "        value = i.find('td')\n",
    "        if key:\n",
    "            \n",
    "            # mergedtoprow defines sections. This is useful to keep track of years\n",
    "            # that scope over the entire section, and labels.\n",
    "            if not i.has_attr('class') or (i.has_attr('class') and (i['class'][0] == 'mergedtoprow')):\n",
    "                sect_change = True\n",
    "                section = ''\n",
    "                section_year = None\n",
    "                \n",
    "                # these are the only things that reliably have sections that we're interested in.\n",
    "                if any(i in key.text.lower() for i in ['gdp', 'population', 'area', \n",
    "                                                       'government', 'resident', 'time']):\n",
    "                    \n",
    "                    # 'largest' avoid issues with New York City, which has a section\n",
    "                    # starting with 'largest_borough_by_area'\n",
    "                    if 'largest' not in key.text.lower():\n",
    "                        section = key.text.lower()\n",
    "                        \n",
    "                        # years are usually in parentheses (2019)\n",
    "                        section_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', section)\n",
    "                        if section_year: section_year = section_year.group(2)\n",
    "                        else:\n",
    "                            section_year = re.search('\\((.*)(-+|\\/+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', section)\n",
    "                            if section_year: section_year = section_year.group(3)\n",
    "                            else:\n",
    "                                section_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(-+|\\/+)?(.*)?\\)', section)\n",
    "                                if section_year: section_year = section_year.group(2)\n",
    "            \n",
    "            if value and value.text.replace(' ', '').replace('\\xa0', '') != '':\n",
    "                                \n",
    "                # get links in values\n",
    "                value_links = value.find_all('a')\n",
    "                if len(value_links) >= 1: \n",
    "                    value_links = [v for v in value_links if v.has_attr('href')]\n",
    "                    value_links = [v['href'] for v in value_links]\n",
    "                    # remove citations:\n",
    "                    value_links = [v for v in value_links if not v.startswith('#')]\n",
    "                else:\n",
    "                    value_links = None\n",
    "\n",
    "                key, value = key.text.lower(), value.text\n",
    "                \n",
    "                # we don't care about rank, and it messes\n",
    "                # up actors like NY state\n",
    "                if sect_change and 'rank' in value.lower():\n",
    "                    continue \n",
    "                    \n",
    "                # Fixes Hangzhou GDP\n",
    "                sc = re.search('((20|19|18)\\d\\d)', value)\n",
    "                fl = re.search(\"[A-Za-z]\", value)\n",
    "                if sect_change and sc:\n",
    "                    section_year = sc.group(1)\n",
    "                    if not fl: continue\n",
    "                \n",
    "                # get any year values in the key or value text.\n",
    "                # these will be associated with this key.\n",
    "                key_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', key)\n",
    "                wh = 'key'\n",
    "                if not key_year:\n",
    "                    key_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', value)\n",
    "                    wh = 'value'\n",
    "                if key_year:\n",
    "                    key_year = key_year.group(2)\n",
    "                \n",
    "\n",
    "                # if we have both a key year and a section year,\n",
    "                # use the key year. \n",
    "                if key_year: year = key_year\n",
    "                else: year = section_year\n",
    "                \n",
    "                # clean up the text.\n",
    "                section = clean(section)\n",
    "                key = clean(key)\n",
    "                #selecting km2 instead of sq mi\n",
    "                if section == 'area' and \"(\" in value:\n",
    "                    temp = value.split(\"(\")\n",
    "                    for val in range(len(temp)):\n",
    "                        if \"km2\" in temp[val] or \"km²\" in temp[val]:\n",
    "                            if type(value) == str:\n",
    "                                value = [v for v in temp[val].strip(\")\").split('\\n')]\n",
    "                    #if area_units is something else\n",
    "                    if 'km2' not in value[0] and \"km²\" not in value[0]:\n",
    "                        value = [temp[0]]\n",
    "                else:\n",
    "                    value = [v for v in value.split('\\n')]\n",
    "                value = [clean(v) for v in value]\n",
    "                #if multiple area types in value\n",
    "                if \"or\" in value[0]:\n",
    "                    temp = value[0].split(\"or\")\n",
    "                    for v in range(len(temp)):\n",
    "                        if \"km2\" in temp[v] or \"km²\" in temp[v]:\n",
    "                            value = [temp[v].strip(\" \")]\n",
    "                \n",
    "                # add section to key, replace spaces.\n",
    "                if section and section != key:\n",
    "                    key = section + ' ' + key\n",
    "                key = key.replace(' ', '_')\n",
    "                key = key.replace('\\xa0', '_')\n",
    "\n",
    "                # add all this info to data dict.\n",
    "                data[key] = value\n",
    "                if year:\n",
    "                    data[key + '_year'] = year\n",
    "                if value_links: \n",
    "                    data[key + '_links'] = value_links\n",
    "        sect_change = False\n",
    "    if 'area' in data:\n",
    "        if \";\" in data['area'][0]:\n",
    "            if \"km2\" in data['area'][0]:\n",
    "                areas = data['area'][0].split(\";\")\n",
    "                chosen = [zz for zz in areas if 'km2' in zz]\n",
    "                area = chosen[0].replace('\\xa0',' ')\n",
    "                area = area.strip(\" \")\n",
    "                data['area'][0] = area\n",
    "        if \"ha\" in data['area'][0]:\n",
    "            try:\n",
    "                area = float(data['area'][0].split(\" \")[0]) / 100\n",
    "                data['area'][0] = str(area) + \" km2\"\n",
    "            except:\n",
    "                data['area'][0] += \" flag\"\n",
    "        if \"sq mi\" in data['area'][0]:\n",
    "            area = float(data['area'][0].split(\" \")[0]) * 2.58999\n",
    "            data['area'][0] = str(area) + \" km2\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def infer_type(categories, title, iso):\n",
    "    city_score = 0\n",
    "    region_score = 0\n",
    "    country_score = 0\n",
    "    company_score = 0\n",
    "    university_score = 0\n",
    "    \n",
    "    cats = [c.lower() for c in categories]\n",
    "    \n",
    "    if any(i in cats for i in ['communities of belgium']):\n",
    "        return ('Region', 10)\n",
    "    \n",
    "    if any(i in cats for i in ['somerset west and taunton', 'alfoz (lugo)', 'araquil']) or  \\\n",
    "        title in ['Sabce', 'Island of Mozambique']:\n",
    "            return ('City', 10)\n",
    "\n",
    "    \n",
    "    cats = ' '.join(cats).split(' ')\n",
    "    if any(i in cats for i in ['births', 'temples']):\n",
    "        return None\n",
    "    \n",
    "    for w in cats:\n",
    "        \n",
    "        if w in ['list', 'lists']: return None\n",
    "                \n",
    "        if w in ['city', 'cities', 'municipality', 'municipalities', 'town', 'towns', 'capital', 'capitals',\n",
    "                 'populated', 'partidos', 'geography', 'places', 'comarcas', 'villages', 'communes', 'wards',\n",
    "                 'ward', 'puebla', 'department', 'areas']:\n",
    "            if w in ['villages', 'cities', 'towns']:\n",
    "                city_score += 2\n",
    "            else:\n",
    "                city_score += 1\n",
    "            \n",
    "        if w in ['arrondissements', 'arrondissement', 'province', 'provinces', 'canton', 'entity',\n",
    "                 'region', 'regions', 'nuts', 'county', 'counties', 'states', 'districts',\n",
    "                 'state', 'territory', 'territories', 'emirate', 'geography', 'voivodeship', 'gmina',\n",
    "                 'canterbury,', 'island', 'district']:\n",
    "            \n",
    "            if iso in ['MUS', 'ZAF', 'TZA', 'ZMB', 'CHN', 'KOR', 'PER', 'TUR'] \\\n",
    "                and w in ['districts', 'district']:\n",
    "                city_score += 1\n",
    "                \n",
    "            elif iso in [\"KOR\", 'USA'] and w in ['county', 'counties']:\n",
    "                city_score += 1\n",
    "                \n",
    "            else:\n",
    "                region_score += 1\n",
    "            \n",
    "        if w in ['country', 'countries', 'states', 'state', 'nations']:\n",
    "            country_score += 1\n",
    "            \n",
    "        if w in ['companies']:\n",
    "            company_score += 1\n",
    "            \n",
    "        if w in ['universities', 'university', 'college', 'colleges', 'educational']:\n",
    "            university_score += 1\n",
    "        \n",
    "        if w in ['disambiguation']:\n",
    "            return 'Disambiguation'\n",
    "                \n",
    "    max_score = max((city_score, region_score, country_score, company_score, university_score))\n",
    "    if max_score == 0: return None\n",
    "    if city_score == max_score: return ('City', max_score)\n",
    "    if company_score == max_score: return ('Company', max_score)\n",
    "    if region_score == max_score: return ('Region', max_score)\n",
    "    if country_score == max_score: return ('Country', max_score)\n",
    "    if university_score == max_score: return ('University', max_score)\n",
    "    \n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = x.replace(',', '')\n",
    "    units = None\n",
    "    if '€' in x:\n",
    "        units = 'EUR'\n",
    "        x = x.replace('€', '')\n",
    "    if '¥' in x:\n",
    "        units = 'RMB'\n",
    "        x = x.replace('¥', '')\n",
    "    if 'CNY' in x:\n",
    "        units = 'RMB'\n",
    "        x = x.replace('CNY', '')\n",
    "    if 'US$' in x:\n",
    "        units = 'USD'\n",
    "        x = x.replace('US$', '')\n",
    "    if '$' in x:\n",
    "        units = 'USD'\n",
    "        x = x.replace('$', '')\n",
    "    if '/km2' in x:\n",
    "        units = '/km2'\n",
    "        x = x.replace('/km2', '')\n",
    "    if 'km2' in x or 'km²' in x:\n",
    "        units = 'km2'\n",
    "        x = x.replace('km2', '').replace('km²', '')\n",
    "    if 'acres' in x:\n",
    "        units = 'acres'\n",
    "        x = x.replace('acres', '')\n",
    "    if 'ha' in x:\n",
    "        units = 'ha'\n",
    "        x = x.replace('ha', '')\n",
    "    if 'km' in x:\n",
    "        units = 'km'\n",
    "        x = x.replace('km', '')\n",
    "    if '/sq mi' in x:\n",
    "        units = '/sq mi'\n",
    "        x = x.replace('/sq mi', '')\n",
    "    if '/sqmi' in x:\n",
    "        units = '/sq mi'\n",
    "        x = x.replace('/sqmi', '')\n",
    "    if 'sq mi' in x:\n",
    "        units = 'sq mi'\n",
    "        x = x.replace('sq mi', '')\n",
    "    if 'sqmi' in x:\n",
    "        units = 'sqmi'\n",
    "        x = x.replace('sq mi', '')\n",
    "    if 'miles' in x:\n",
    "        units = 'mi'\n",
    "        x = x.replace('miles', '')\n",
    "    if 'mi' in x and 'million' not in x:\n",
    "        units = 'mi'\n",
    "        x = x.replace('mi', '')\n",
    "    if 'm' in x and 'mi' not in x and 'million' not in x:\n",
    "        units = 'm'\n",
    "        x = x.replace('m', '')\n",
    "    if 'ft' in x:\n",
    "        units = 'ft'\n",
    "        x = x.replace('ft', '')\n",
    "    \n",
    "    factor = 1\n",
    "    if 'thousand' in x: factor = 1000\n",
    "    if 'million' in x: factor  = 1000000\n",
    "    if 'billion' in x: factor  = 1000000000\n",
    "    if 'trillion' in x: factor = 1000000000000\n",
    "    x = x.replace('thousand', '').replace('million', '').replace('billion', '').replace('trillion', '')\n",
    "            \n",
    "    x = [c for c in x if c in '1234567890.']\n",
    "    x = ''.join(x)\n",
    "    x = x.strip('.')\n",
    "    if x == '': return None, None\n",
    "    if x.count(\".\") > 1: return x, \"flag\"\n",
    "    return float(x) * factor, units\n",
    "\n",
    "\n",
    "#### In[8]:\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def process_page(page):\n",
    "    data = {}\n",
    "    \n",
    "    data['title']   = page.find('h1', {'class': 'firstHeading'}).text\n",
    "    if data['title'].startswith('File:'): return None\n",
    "    ## print(data['title'])\n",
    "    \n",
    "    # get coords in decimal degrees\n",
    "    latlon = page.find('span', {'class': 'geo'})\n",
    "    if latlon and \",\" not in str(latlon.text): \n",
    "        data['lat'] = float(latlon.text.split(\";\")[0])\n",
    "        data['lng'] = float(latlon.text.split(\";\")[1])\n",
    "    \n",
    "    \n",
    "    # attempt to get a wiki_iso. \n",
    "    data['wiki_iso'] = None\n",
    "    infobox = page.find('table', {'class': 'infobox'})\n",
    "    if infobox:\n",
    "        for i in infobox.find_all('a'):\n",
    "            if i.has_attr('title'):\n",
    "                title = i['title']\n",
    "                if title in title_iso:\n",
    "                    data['wiki_iso'] = title_iso[title]\n",
    "                    break\n",
    "                    \n",
    "    \n",
    "    # Fixing for some places\n",
    "    if \"Hong Kong\" in data['title']: data['wiki_iso'] = 'HKG'\n",
    "    if \"Zimbabwe\" in data['title']: data['wiki_iso'] = 'ZWE'\n",
    "    if \"Waterloo (village), New York\" in data['title']: data['wiki_iso'] = 'USA'\n",
    "    \n",
    "        \n",
    "        \n",
    "    if not data['wiki_iso']:\n",
    "        ps = [p for p in page.find_all('p') if not p.has_attr('class')]\n",
    "        if len(ps) >= 1:\n",
    "            for i in ps[0].find_all('a'):\n",
    "                if i.has_attr('title'):\n",
    "                    title = i['title']\n",
    "                    if title in title_iso:\n",
    "                        data['wiki_iso'] = title_iso[title]\n",
    "                        break\n",
    "    \n",
    "    data['infobox'] = get_infobox(page)\n",
    "    \n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        if 'country' in data['infobox']:\n",
    "            if data['infobox']['country'][0] in title_iso:\n",
    "                data['wiki_iso'] = title_iso[data['infobox']['country'][0]]\n",
    "    \n",
    "    # These 3 introduce a lot of noise. Indiana -> India, for example. \n",
    "    # necessary for some actors though. :(\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        ps = [p for p in page.find_all('p') if not p.has_attr('class')]\n",
    "        if len(ps) >= 1:\n",
    "            for k, v in title_iso.items():\n",
    "                if k in ps[0].text:\n",
    "                    data['wiki_iso'] = v\n",
    "                    break\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        if infobox:\n",
    "            for k, v in title_iso.items():\n",
    "                if k in infobox.text:\n",
    "                    data['wiki_iso'] = v\n",
    "                    break\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        for k, v in title_iso.items():\n",
    "            if k in data['title']:\n",
    "                data['wiki_iso'] = v\n",
    "                break\n",
    "    \n",
    "    data['categories'] = []\n",
    "    cat = page.find('div', {'id': 'mw-normal-catlinks'})\n",
    "    if cat:\n",
    "        data['categories'] = [i.text for i in cat.find_all('li')]\n",
    "    if 'category_label' in data['infobox']:\n",
    "        data['categories'].append(data['infobox']['category_label'])  \n",
    "    data['wiki_type'] = infer_type(data['categories'], data['title'], data['wiki_iso'])\n",
    "    \n",
    "        \n",
    "    if type(data['wiki_type']) == tuple : data['wiki_type'] = data['wiki_type'][0]\n",
    "    \n",
    "\n",
    "    \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'area' in k and 'links' not in k and 'year' not in k and 'code' not in k \\\n",
    "            and 'coordinates' not in k and k != 'areas' and 'rank' not in k:\n",
    "            data['area'] = data['infobox'][k]        \n",
    "            data['area'], data['area_units'] = clean_numbers(data['area'][0])\n",
    "            if k+'_year' in data['infobox']:\n",
    "                data['area_year'] = data['infobox'][k+'_year']\n",
    "            break  \n",
    "            \n",
    "    data['population'] = []\n",
    "\n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'population' in k and 'links' not in k and 'year' not in k and 'municipality' not in k:\n",
    "            if 'rank' not in k and 'density' not in k and 'percent' not in k and 'ethnic' not in k \\\n",
    "                and 'change' not in k and 'demonym' not in k and '_by_' not in k and 'median' not in k \\\n",
    "                and 'household' not in k and v[0] != '' and 'largest' not in k and 'for' not in v[0] \\\n",
    "                and 'language' not in k and not k.endswith('population_') and 'gender' not in k \\\n",
    "                and 'pop_2011–2016' not in k: \n",
    "                    if 'estimate' in k and k+'_year' in data['infobox'].keys():\n",
    "                        yy = data['infobox'][k+'_year']\n",
    "                        data['population'].append([yy+'_'+k,data['infobox'][k]])\n",
    "                    else:\n",
    "                        data['population'].append([k,data['infobox'][k]])\n",
    "                    for pop in data['population']:\n",
    "                        if type(pop[1]) != float and type(pop[1]) != 'NoneType' and pop[1][0] != \"N/A\":\n",
    "                            pop[1], _ = clean_numbers(pop[1][0])\n",
    "                    \n",
    "    if data['population']:\n",
    "        k = data['population'][0][0]\n",
    "        if k+'_year' in data['infobox'].keys():\n",
    "            data['population_year'] = data['infobox'][k+'_year']\n",
    "        for tem in data['population']:\n",
    "            tem[0] = tem[0].replace(',','')\n",
    "            tem[0] = tem[0].replace('population_', '')\n",
    "            tem[0] = tem[0].replace('greater_toronto_area','metro')\n",
    "            tem[0] = tem[0].replace('†_city_proper._', '')\n",
    "            if \"(\" in tem[0]: tem[0] = tem[0].split(\"_\")[-1]\n",
    "            if 'population_year' not in data:\n",
    "                if 'census' in tem[0]:\n",
    "                    data['population_year'] = tem[0].split(\"_\")[0]\n",
    "                elif 'estimate' in tem[0]:\n",
    "                    data['population_year'] = tem[0].split(\"_\")[0]\n",
    "        #data['population'][0][0] = 'population'\n",
    "    else:\n",
    "        del data['population']\n",
    "    \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'gdp' in k and 'links' not in k and 'year' not in k and 'hdi' not in k:\n",
    "            data['gdp'] = data['infobox'][k]\n",
    "            if 'USD' in data['gdp'][0] and 'CNY' in data['gdp'][0]:\n",
    "                data['gdp'] = [\"USD\" + data['gdp'][0].split(\"USD\")[1]]\n",
    "            data['gdp'], data['gdp_units'] = clean_numbers(data['gdp'][0])\n",
    "            if k+'_year' in data['infobox']:\n",
    "                data['gdp_year'] = data['infobox'][k+'_year']\n",
    "            break\n",
    "                \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'elevation' in k and 'links' not in k and 'year' not in k:\n",
    "            data['elevation'] = data['infobox'][k][0]\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    " \n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def get_region_hierarchy(data):\n",
    "    \n",
    "    # generate a list of region triples. \n",
    "    # (key, page['title'], page['area'], page['population'])\n",
    "    \n",
    "    region_words = ['region', 'state', 'province', 'county', 'emirate', 'gmina', 'island',\n",
    "                    'district', 'raion', 'prefecture', 'territory', 'canton',\n",
    "                    'governorate', 'administrative_region', 'mkhare', 'metropolitan_city',\n",
    "                    'oblast', 'autonomous_region', 'commune', 'entity', 'sovereign_state',\n",
    "                    'municipality', 'provinces', 'marz', 'republic', 'cercle',\n",
    "                    'autonomous_community', 'provincekind', 'governorates', 'comarca',\n",
    "                    'locale', 'subdivision', 'voblast', 'voivodeship', 'parish',\n",
    "                    'constituent_country', 'ceremonial_county', 'arrondissement', 'admin', 'country']\n",
    "    \n",
    "    tmp = []\n",
    "    for k,v in data['infobox'].items():\n",
    "        if '_links' in k or '_year' in k or 'postcode' in k or 'area_' in k or 'population_' in k or 'gdp_' in k \\\n",
    "            or 'congressional_' in k or 'government_' in k or 'highest_' in k or '_electorate' in k or 'nuts_' in k \\\n",
    "            or 'historical_' in k or '_bird' in k or '_fish' in k or '_flower' in k or '_stone' in k or 'animal' in k \\\n",
    "            or 'language' in k or 'historic_' in k or \"_specialized\" in k or \"judicial_\" in k or \"romanisation\" in k: continue\n",
    "        if k.startswith(\"sub\") and k.endswith('s'): continue\n",
    "        if any(w in k for w in region_words):\n",
    "            for i in v:\n",
    "                if any(char.isdigit() for char in i) or i.lower().startswith(\"list\") or \",\" in i: break\n",
    "                if i != '' and i != 'n':\n",
    "                    tmp += [[k, i]]\n",
    "                    \n",
    "    if len(tmp) > 0 and ('country' in tmp[-1][0] or 'sovereign_state' in tmp[-1][0]):\n",
    "        return tmp[::-1]\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def handle_disambig(name, iso, entity_type, disambig, debug=False):\n",
    "    page = get_page(disambig['title'])\n",
    "    page = page.find('div', {'class': 'mw-parser-output'})\n",
    "    if debug: print('Disambiguating')\n",
    "    if page:\n",
    "        for i in page.find_all('li'):\n",
    "            a = i.find('a')\n",
    "            if a and a.has_attr('title'):\n",
    "                data = process_page(get_page(a['title']))\n",
    "                if debug: print(data['title'], data['wiki_type'], data['wiki_iso'])\n",
    "                if data['wiki_type'] == entity_type and data['wiki_iso'] == iso:\n",
    "                    data['region_hierarchy'] = get_region_hierarchy(data)\n",
    "                    if data['infobox'] == {} or data['infobox'] == None:\n",
    "                        if debug: print('no infobox')\n",
    "\n",
    "                    del data['infobox']\n",
    "                    data['categories'] = '; '.join(data['categories'])\n",
    "                    return data\n",
    "    return {}\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def get_data(name, iso, entity_type, check_country=True, debug=False, check_disambig=True):\n",
    "    name = name.replace('!', '')\n",
    "    name = name.replace('Aggregazione ', '')\n",
    "    name = name.replace('Intercommunality of ', '')\n",
    "    name = name.replace('Ilha de', \"Island of\")\n",
    "    if \",\" in name or 'State' in name or iso_title[iso] in name or name in ['Tlokwe']:\n",
    "        if iso == 'JPN': \n",
    "            if \"Town\" in name: name = name.replace(\"Town\", '')\n",
    "            if \"City\" in name: name = name.split(\" City\")[0]\n",
    "        titles = search(name)\n",
    "    else: \n",
    "        if name.startswith('Nasu'): name = name.replace(\"-\",'').title()\n",
    "        \n",
    "        titles = search(name + \", \" + iso_title[iso])\n",
    "    if not check_country and debug: print(titles)\n",
    "    pages = get_pages(titles)\n",
    "    if not pages:\n",
    "        if debug: print('pages null')\n",
    "        return {}\n",
    "    pages = [process_page(page) for page in pages]\n",
    "    \n",
    "    #for p in pages: print(p['title'])\n",
    "    #debug = True\n",
    "    \n",
    "    if debug:\n",
    "        for p in pages: print(p)\n",
    "    \n",
    "\n",
    "    if len(pages) != 0:\n",
    "        disambig = [i for i in pages if i['wiki_type'] == 'Disambiguation']\n",
    "        if len(disambig) > 0: disambig = disambig[0]\n",
    "        else: disambig = None\n",
    "        \n",
    "        # filter iso\n",
    "        if iso:\n",
    "            iso_pages = [i for i in pages if 'wiki_iso' in i]\n",
    "            iso_pages = [i for i in iso_pages if i['wiki_iso'] == iso]\n",
    "\n",
    "            if len(iso_pages) == 0:\n",
    "                name = name.lower().replace('dimos', 'municipality')\n",
    "                if disambig and check_disambig:\n",
    "                    d = handle_disambig(name, iso, entity_type, disambig, debug=debug)\n",
    "                    if d and d != {}: return d\n",
    "                if check_country and iso in iso_title:\n",
    "                    if debug: print('\\nchecking country')\n",
    "                    name = name.replace('d\\'', '')\n",
    "                    return get_data(name + \", \" + iso_title[iso], iso, entity_type, check_country=False, debug=debug)\n",
    "        else: iso_pages = pages\n",
    "        \n",
    "        \n",
    "        #for i in iso_pages: print(i['title'], i['wiki_type'])\n",
    "        #filter entity_type\n",
    "        et_pages = [i for i in iso_pages if i['wiki_type'] == entity_type]  \n",
    "     \n",
    "        if len(et_pages) == 0:\n",
    "            name = name.lower().replace('dimos', 'municipality')\n",
    "            if disambig and check_disambig:\n",
    "                d = handle_disambig(name, iso, entity_type, disambig, debug=debug)\n",
    "                if d and d != {}: return d\n",
    "            if check_country and iso in iso_title:\n",
    "                if debug: print('\\nchecking country')\n",
    "                name = name.replace('d\\'', '')\n",
    "                return get_data(name + \", \" + iso_title[iso], iso, entity_type, check_country=False, debug=debug)\n",
    "        else:\n",
    "            max_score = 0\n",
    "            rank = 0\n",
    "            # for page_num in range(len(et_pages)):\n",
    "            #     score = infer_type(et_pages[page_num]['categories'],\n",
    "            #                        et_pages[page_num]['title'],\n",
    "            #                        et_pages[page_num]['wiki_iso'])[1]\n",
    "            #     print(et_pages[page_num]['title'], score)\n",
    "            #     if score > max_score:\n",
    "            #         max_score = score\n",
    "            #         rank = page_num\n",
    "            data = et_pages[rank]\n",
    "            data['region_hierarchy'] = get_region_hierarchy(data)\n",
    "        \n",
    "            if data['infobox'] == {} or data['infobox'] == None:\n",
    "                if debug: print('no infobox')\n",
    "\n",
    "            del data['infobox']\n",
    "            data['categories'] = '; '.join(data['categories'])\n",
    "            \n",
    "            return data\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Ann Arbor, Michigan',\n",
       " 'lat': 42.28139,\n",
       " 'lng': -83.74833,\n",
       " 'wiki_iso': 'USA',\n",
       " 'categories': 'Ann Arbor, Michigan; Populated places established in 1824; Academic enclaves; County seats in Michigan; Cities in Washtenaw County, Michigan; Metro Detroit; 1824 establishments in Michigan Territory; University towns in the United States; City',\n",
       " 'wiki_type': 'City',\n",
       " 'area': 74.56,\n",
       " 'area_units': 'km2',\n",
       " 'population': [['city', 113934.0],\n",
       "  ['2019_estimate', 119980.0],\n",
       "  ['urban', 306022.0],\n",
       "  ['metro', 344791.0]],\n",
       " 'population_year': '2010',\n",
       " 'elevation': '840 ft',\n",
       " 'region_hierarchy': [['country', 'United States'],\n",
       "  ['state', 'Michigan'],\n",
       "  ['county', 'Washtenaw']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing cell\n",
    "\n",
    "get_data(\"Ann Arbor\", \"USA\", \"City\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing in Region Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reg_dict.csv') as x:\n",
    "    reg_dict = {}\n",
    "    reader = csv.reader(x)\n",
    "    skip = next(reader)\n",
    "    for row in reader:\n",
    "        reg_dict[row[1]] = row[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past Usages of Scraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding _Latitude & Longitude, Admin_1_ and _Population Information_ from Wikipedia and Flagging Rows with Possible Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your file name? Current_ICLEI_Members_list_scrape.csv\n",
      "0.0% completed\n",
      "1.86% completed\n",
      "3.73% completed\n",
      "5.59% completed\n",
      "7.45% completed\n",
      "9.32% completed\n",
      "11.18% completed\n",
      "13.04% completed\n",
      "14.91% completed\n",
      "16.77% completed\n",
      "18.63% completed\n",
      "20.5% completed\n",
      "22.36% completed\n",
      "24.22% completed\n",
      "26.09% completed\n",
      "27.95% completed\n",
      "29.81% completed\n",
      "31.68% completed\n",
      "33.54% completed\n",
      "35.4% completed\n",
      "37.27% completed\n",
      "39.13% completed\n",
      "40.99% completed\n",
      "42.86% completed\n",
      "44.72% completed\n",
      "46.58% completed\n",
      "48.45% completed\n",
      "50.31% completed\n",
      "52.17% completed\n",
      "54.04% completed\n",
      "55.9% completed\n",
      "57.76% completed\n",
      "59.63% completed\n",
      "61.49% completed\n",
      "63.35% completed\n",
      "65.22% completed\n",
      "67.08% completed\n",
      "68.94% completed\n",
      "70.81% completed\n",
      "72.67% completed\n",
      "74.53% completed\n",
      "76.4% completed\n",
      "78.26% completed\n",
      "80.12% completed\n",
      "81.99% completed\n",
      "83.85% completed\n",
      "85.71% completed\n",
      "87.58% completed\n",
      "89.44% completed\n",
      "91.3% completed\n",
      "93.17% completed\n",
      "95.03% completed\n",
      "96.89% completed\n",
      "98.76% completed\n",
      "\n",
      "Would you like to flag possible rows with errors? (Y/N) y\n",
      "47 rows have been flagged!\n",
      "\n",
      "Would you like to export the file? (Y/N) y\n",
      "\n",
      "File has been updated!\n",
      "Rows with errors: []\n"
     ]
    }
   ],
   "source": [
    "filename = input(\"What is your file name? \") #latlngscrape.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 6 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = 'NA'\n",
    "    x['wiki_lat'] = 'NA'\n",
    "    x['wiki_lng'] = 'NA'\n",
    "    x['wiki_pop'] = 'NA'\n",
    "    x['wiki_pop_year'] = 'NA'\n",
    "    x['wiki_state'] = 'NA'\n",
    "    x['admin_1'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:   \n",
    "            if 'lat' in info:\n",
    "                x['wiki_lat'] = info['lat']\n",
    "                x['wiki_lng'] = info['lng']\n",
    "            x['wiki_name'] = info['title']\n",
    "            if 'population' in info:\n",
    "                x['wiki_pop'] = info['population']\n",
    "                if 'population_year' in info: x['wiki_pop_year'] = info['population_year']\n",
    "            if info['region_hierarchy']:\n",
    "                x['admin_1'] = info['region_hierarchy'][-1]\n",
    "                for reg in info['region_hierarchy']:\n",
    "                    if reg[0] == 'state':\n",
    "                        x['wiki_state'] = reg[1]\n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "print()\n",
    "d1 = input(\"Would you like to flag possible rows with errors? (Y/N) \")\n",
    "if d1.lower() == 'y':\n",
    "    for x in con:\n",
    "        x['flag'] = 1\n",
    "        name = x['name'].replace(\",\", \" \")\n",
    "        name = name.replace(\"San\", \"\")\n",
    "        name = name.split()\n",
    "        wname = x['wiki_name'].replace(\",\", \" \").split()\n",
    "        if any(word in name for word in wname):\n",
    "            x['flag'] = 0\n",
    "    \n",
    "    a = 0\n",
    "    for x in con:\n",
    "        a += x['flag']\n",
    "                \n",
    "    print(\"%s rows have been flagged!\" % a) \n",
    "    print()\n",
    "\n",
    "d2 = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if d2.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print()\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print()\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding _Latitude & Longitude Information_ from Wikipedia and Flagging Rows with Possible Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = input(\"What is your file name? \") #latlngscrape.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 6 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = 'NA'\n",
    "    x['wiki_lat'] = 'NA'\n",
    "    x['wiki_lng'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:   \n",
    "            if 'lat' in info:\n",
    "                x['wiki_lat'] = info['lat']\n",
    "                x['wiki_lng'] = info['lng']\n",
    "            x['wiki_name'] = info['title']\n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "print()\n",
    "d1 = input(\"Would you like to flag possible rows with errors? (Y/N) \")\n",
    "if d1.lower() == 'y':\n",
    "    for x in con:\n",
    "        x['flag'] = 1\n",
    "        name = x['name'].replace(\",\", \" \")\n",
    "        name = name.replace(\"San\", \"\")\n",
    "        name = name.split()\n",
    "        wname = x['wiki_name'].replace(\",\", \" \").split()\n",
    "        if any(word in name for word in wname):\n",
    "            x['flag'] = 0\n",
    "    \n",
    "    a = 0\n",
    "    for x in con:\n",
    "        a += x['flag']\n",
    "                \n",
    "    print(\"%s rows have been flagged!\" % a) \n",
    "    print()\n",
    "\n",
    "d2 = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if d2.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print()\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print()\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding _Population, Latitude & Longitude Information_ from Wikipedia and Flagging Rows with Possible Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = input(\"What is your file name? \") #errorneous_population_year.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 5 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = 'NA'\n",
    "    x['wiki_lat'] = 'NA'\n",
    "    x['wiki_lng'] = 'NA'\n",
    "    x['wiki_pop'] = 'NA'\n",
    "    x['wiki_pop_year'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:   \n",
    "            if 'lat' in info:\n",
    "                x['wiki_lat'] = info['lat']\n",
    "                x['wiki_lng'] = info['lng']\n",
    "            x['wiki_name'] = info['title']\n",
    "            if 'population' in info:\n",
    "                x['wiki_pop'] = info['population']\n",
    "                if 'population_year' in info: x['wiki_pop_year'] = info['population_year']\n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "print()\n",
    "d1 = input(\"Would you like to flag possible rows with errors? (Y/N) \")\n",
    "if d1.lower() == 'y':\n",
    "    for x in con:\n",
    "        x['flag'] = 1\n",
    "        name = x['name'].replace(\",\", \" \")\n",
    "        name = name.replace(\"San\", \"\")\n",
    "        name = name.split()\n",
    "        wname = x['wiki_name'].replace(\",\", \" \").split()\n",
    "        if any(word in name for word in wname):\n",
    "            x['flag'] = 0\n",
    "\n",
    "    \n",
    "    for x in con:\n",
    "        if x['flag'] == 0 and x['lat'] != 'NA' and x['wiki_lat'] != 'NA':\n",
    "            if abs(float(x['lat'])-float(x['wiki_lat'])) > 0.1 \\\n",
    "                or abs(float(x['lng'])-float(x['wiki_lng'])) > 0.1:\n",
    "                    x['flag'] = 1\n",
    "    \n",
    "    a = 0\n",
    "    for x in con:\n",
    "        a += x['flag']\n",
    "                \n",
    "    print(\"%s rows have been flagged!\" % a) \n",
    "    print()\n",
    "\n",
    "d2 = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if d2.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print()\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print()\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding _Population, Latitude & Longitude Information_ from Wikipedia and Flagging Rows with Possible Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = input(\"What is your file name? \") #population_scraping.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 8 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = 'NA'\n",
    "    x['wiki_lat'] = 'NA'\n",
    "    x['wiki_lng'] = 'NA'\n",
    "    x['wiki_pop'] = 'NA'\n",
    "    x['wiki_pop_year'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:   \n",
    "            if 'lat' in info:\n",
    "                x['wiki_lat'] = info['lat']\n",
    "                x['wiki_lng'] = info['lng']\n",
    "            x['wiki_name'] = info['title']\n",
    "            if 'population' in info:\n",
    "                x['wiki_pop'] = info['population']\n",
    "                if 'population_year' in info: x['wiki_pop_year'] = info['population_year']\n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "print()\n",
    "d1 = input(\"Would you like to flag possible rows with errors? (Y/N) \")\n",
    "if d1.lower() == 'y':\n",
    "    for x in con:\n",
    "        x['flag'] = 1\n",
    "        name = x['name'].replace(\",\", \" \")\n",
    "        name = name.replace(\"San\", \"\")\n",
    "        name = name.split()\n",
    "        wname = x['wiki_name'].replace(\",\", \" \").split()\n",
    "        if any(word in name for word in wname):\n",
    "            x['flag'] = 0\n",
    "\n",
    "    \n",
    "    for x in con:\n",
    "        if x['flag'] == 0 and x['lat'] != 'NA' and x['wiki_lat'] != 'NA':\n",
    "            if abs(float(x['lat'])-float(x['wiki_lat'])) > 0.1 \\\n",
    "                or abs(float(x['lng'])-float(x['wiki_lng'])) > 0.1:\n",
    "                    x['flag'] = 1\n",
    "    \n",
    "    a = 0\n",
    "    for x in con:\n",
    "        a += x['flag']\n",
    "                \n",
    "    print(\"%s rows have been flagged!\" % a) \n",
    "    print()\n",
    "\n",
    "d2 = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if d2.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print()\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print()\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding _Region Information_ from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = input(\"What is your file name? \") #subnational_contextuals_database_August2020.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 14 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = 'NA'\n",
    "    x['region_hierarchy'] = 'NA'\n",
    "    x['region_name'] = 'NA'\n",
    "    x['admin_1'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:   \n",
    "            if 'lat' in info:\n",
    "                lat = info['lat']\n",
    "                lng = info['lng']\n",
    "                if float(x['lat'])-0.5 > float(lat) or float(lat) > float(x['lat'])+0.5 or float(x['lng'])-0.5 > float(lng) or float(lng) > float(x['lng'])+0.5: \n",
    "                    continue\n",
    "            x['wiki_name'] = info['title']\n",
    "            if info['region_hierarchy']:\n",
    "                x['region_hierarchy'] = info['region_hierarchy']\n",
    "                x['admin_1'] = info['region_hierarchy'][-1]\n",
    "                if iso in reg_dict:\n",
    "                    for reg in info['region_hierarchy']:\n",
    "                        if reg[0] in reg_dict[iso].lower():\n",
    "                            x['region_name'] = reg\n",
    "                else:\n",
    "                    x['region_name'] = x['admin_1'] \n",
    "            \n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "print()\n",
    "decision = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if decision.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print()\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print()\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating _Area, Population, GDP_ NA Values & Adding _Latitude, Longitude, Elevation and Region Information_ from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = input(\"What is your file name? \") #contextuals_database_10June20_zy.csv\n",
    "\n",
    "with open(filename, encoding = 'utf-8-sig') as x:\n",
    "    con = []\n",
    "    reader = csv.DictReader(x)\n",
    "    for rows in reader:\n",
    "        con.append(dict(rows))\n",
    "\n",
    "errors = []\n",
    "a = -1\n",
    "\n",
    "for x in con:\n",
    "    a += 1\n",
    "    if a % 14 == 0: print((str(round(a/len(con) * 100,2)) + \"% completed\"))\n",
    "    name = x['name']\n",
    "    iso = x['iso']\n",
    "    et = x['entity_type']\n",
    "    x['wiki_name'] = \"NA\"\n",
    "    x['wiki_lat'] = \"NA\"\n",
    "    x['wiki_lng'] = \"NA\"\n",
    "    x['elevation'] = \"NA\"\n",
    "    x['region_hierarchy'] = 'NA'\n",
    "    x['region_name'] = 'NA'\n",
    "    x['admin_1'] = 'NA'\n",
    "    try:\n",
    "        info = get_data(name, iso, et)\n",
    "        if info:\n",
    "            if x['area'] == \"NA\":\n",
    "                if 'area' in info:\n",
    "                    x['area'] = info['area']\n",
    "                    if info['area_units']: x['area_units'] = info['area_units']\n",
    "            if x['population'] == \"NA\":\n",
    "                if 'population' in info:\n",
    "                    x['population'] = info['population']\n",
    "                    if info['population_year']: x['population_year'] = info['population_year']\n",
    "            if x['gdp'] == \"NA\":\n",
    "                if 'gdp' in info:\n",
    "                    x['gdp'] = info['gdp']\n",
    "                    if info['gdp_unit']: x['gdp_unit'] = info['gdp_unit']\n",
    "                    if info['gdp_year']: x['gdp_year'] = info['gdp_year']    \n",
    "            x['wiki_name'] = info['title']\n",
    "            if \"lat\" in info:\n",
    "                x['wiki_lat'] = info['lat']\n",
    "                x['wiki_lng'] = info['lng']\n",
    "            if 'elevation' in info:\n",
    "                x['elevation'] = info['elevation']\n",
    "            if info['region_hierarchy']:\n",
    "                x['region_hierarchy'] = info['region_hierarchy']\n",
    "                x['admin_1'] = info['region_hierarchy'][-1]\n",
    "                if iso in reg_dict:\n",
    "                    for reg in info['region_hierarchy']:\n",
    "                        if reg[0] in reg_dict[iso].lower():\n",
    "                            x['region_name'] = reg\n",
    "                else:\n",
    "                    x['region_name'] = x['admin_1']  \n",
    "            \n",
    "            \n",
    "    except:\n",
    "        errors.append(a)\n",
    "        print(errors)\n",
    "\n",
    "\n",
    "decision = input(\"Would you like to export the file? (Y/N) \" )\n",
    "\n",
    "if decision.lower() == \"y\":\n",
    "    with open(filename.strip(\".csv\") + \"_updated.csv\", 'w', newline='', encoding = 'utf-8-sig') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "        header = list(con[0].keys())\n",
    "        spamwriter.writerow(header)\n",
    "        for x in con:\n",
    "            row = list(x.values())\n",
    "            spamwriter.writerow(row)\n",
    "    print(\"File has been updated!\")\n",
    "    print(\"Rows with errors: \" + str(errors))\n",
    "    \n",
    "else:\n",
    "    print(\"Rows with errors: \" + str(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper for other Languages (Work In Progress)\n",
    "#### Includes: Spanish, Ukranian, Hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_2(title, iso):\n",
    "    global lan\n",
    "    \n",
    "    if title is not None:\n",
    "        # retry page processing as sometimes network errors\n",
    "        # can occur. Most of the time an exception will not be thrown.\n",
    "        if iso == \"ESP\": lan = 'es'\n",
    "        elif iso == \"UKR\": lan = 'uk'\n",
    "        elif iso == \"HUN\": lan = 'hu'\n",
    "\n",
    "        try:\n",
    "            page = requests.get('https://' + lan + '.wikipedia.org/wiki/' + title)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                page = requests.get('https://' + lan + '.wikipedia.org/wiki/' + title)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                return None\n",
    "        return BeautifulSoup(page.content, 'lxml')\n",
    "    \n",
    "def get_pages_2(titles, iso):\n",
    "    \n",
    "    if type(titles) == list and len(titles) > 0:\n",
    "        pgs = []\n",
    "        for title in titles:\n",
    "            pg = get_page_2(title, iso)\n",
    "            if pg: pgs.append(pg)\n",
    "            \n",
    "        return pgs\n",
    "    \n",
    "def search_2(name, iso):\n",
    "    \"\"\"\n",
    "    queries the wikimedia API to return {limit} matches to the search term\n",
    "    \"\"\"\n",
    "    titles = _search_query_2(name, iso)\n",
    "    return titles\n",
    "    \n",
    "def _search_query_2(name, iso):\n",
    "    \"\"\"\n",
    "    the underlying api query that powers the search function.\n",
    "    \"\"\"\n",
    "    if iso == \"ESP\": lan = 'es'\n",
    "    elif iso == \"UKR\": lan = 'uk'\n",
    "    elif iso == \"HUN\": lan = 'hu'\n",
    "    else: return None\n",
    "    \n",
    "    url = 'https://' + lan + '.wikipedia.org/w/index.php'\n",
    "    API_URL = 'https://' + lan + '.wikipedia.org/w/api.php'\n",
    "    \n",
    "    params = {\n",
    "        'sort': 'relevance',\n",
    "        'search': name,\n",
    "        'profile': 'advanced',\n",
    "        'fulltext': 1\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    if r.status_code == 200:\n",
    "        page = BeautifulSoup(r.content, 'lxml')\n",
    "        titles = page.find_all('div', {'class': 'mw-search-result-heading'})\n",
    "        titles = [t.find('a')['title'] for t in titles]\n",
    "        return titles\n",
    "    else:\n",
    "        time.sleep(0.5)\n",
    "        r = requests.get(API_URL, params=params)\n",
    "        return titles\n",
    "\n",
    "def get_infobox_2(page):\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    infobox = page.find('table', {'class': 'infobox'})\n",
    "    if not infobox:\n",
    "        return {}\n",
    "\n",
    "    \n",
    "    fix_newlines(infobox)\n",
    "    data = get_top_region(infobox, data)\n",
    "    data = get_cats(infobox, data)\n",
    "    \n",
    "    section = ''\n",
    "    section_year = None\n",
    "    sect_change = False\n",
    "    \n",
    "    # iterate through each table row.\n",
    "    for i in infobox.find_all('tr'):\n",
    "        key = i.find('th')\n",
    "        value = i.find('td')\n",
    "        if key:\n",
    "            \n",
    "            # mergedtoprow defines sections. This is useful to keep track of years\n",
    "            # that scope over the entire section, and labels.\n",
    "            if not i.has_attr('class') or (i.has_attr('class') and (i['class'][0] == 'mergedtoprow')):\n",
    "                sect_change = True\n",
    "                section = ''\n",
    "                section_year = None\n",
    "                \n",
    "                # these are the only things that reliably have sections that we're interested in.\n",
    "                if any(i in key.text.lower() for i in ['gdp', 'population', 'area', 'surface',\n",
    "                                                       'government', 'resident', 'time']):\n",
    "                    \n",
    "                    # 'largest' avoid issues with New York City, which has a section\n",
    "                    # starting with 'largest_borough_by_area'\n",
    "                    if 'largest' not in key.text.lower():\n",
    "                        section = key.text.lower()\n",
    "                        \n",
    "                        # years are usually in parentheses (2019)\n",
    "                        section_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', section)\n",
    "                        if section_year: section_year = section_year.group(2)\n",
    "                        else:\n",
    "                            section_year = re.search('\\((.*)(-+|\\/+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', section)\n",
    "                            if section_year: section_year = section_year.group(3)\n",
    "                            else:\n",
    "                                section_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(-+|\\/+)?(.*)?\\)', section)\n",
    "                                if section_year: section_year = section_year.group(2)\n",
    "            \n",
    "            if value and value.text.replace(' ', '').replace('\\xa0', '') != '':\n",
    "                                \n",
    "                # get links in values\n",
    "                value_links = value.find_all('a')\n",
    "                if len(value_links) >= 1: \n",
    "                    value_links = [v for v in value_links if v.has_attr('href')]\n",
    "                    value_links = [v['href'] for v in value_links]\n",
    "                    # remove citations:\n",
    "                    value_links = [v for v in value_links if not v.startswith('#')]\n",
    "                else:\n",
    "                    value_links = None\n",
    "\n",
    "                key, value = key.text.lower(), value.text\n",
    "                \n",
    "                # we don't care about rank, and it messes\n",
    "                # up actors like NY state\n",
    "                if sect_change and 'rank' in value.lower():\n",
    "                    continue \n",
    "                    \n",
    "                # Fixes Hangzhou GDP\n",
    "                sc = re.search('((20|19|18)\\d\\d)', value)\n",
    "                fl = re.search(\"[A-Za-z]\", value)\n",
    "                if sect_change and sc:\n",
    "                    section_year = sc.group(1)\n",
    "                    if not fl: continue\n",
    "                \n",
    "                # get any year values in the key or value text.\n",
    "                # these will be associated with this key.\n",
    "                key_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', key)\n",
    "                wh = 'key'\n",
    "                if not key_year:\n",
    "                    key_year = re.search('\\((.*\\s+)?(\\d\\d\\d\\d)(\\s+.*)?\\)', value)\n",
    "                    wh = 'value'\n",
    "                if key_year:\n",
    "                    key_year = [key_year.group(2)]\n",
    "                \n",
    "\n",
    "                # if we have both a key year and a section year,\n",
    "                # use the key year. \n",
    "                if key_year: year = key_year\n",
    "                else: year = section_year\n",
    "                \n",
    "                # clean up the text.\n",
    "                section = clean(section)\n",
    "                key = clean(key)\n",
    "                #selecting km2 instead of sq mi\n",
    "                if section == 'area' or section == 'surface' and \"(\" in value:\n",
    "                    temp = value.split(\"(\")\n",
    "                    for val in range(len(temp)):\n",
    "                        if \"km2\" in temp[val] or \"km²\" in temp[val]:\n",
    "                            if type(value) == str:\n",
    "                                value = [v for v in temp[val].strip(\")\").split('\\n')]\n",
    "                    #if area_units is something else\n",
    "                    if 'km2' not in value[0] and \"km²\" not in value[0]:\n",
    "                        value = [temp[0]]\n",
    "                else:\n",
    "                    value = [v for v in value.split('\\n')]\n",
    "                value = [clean(v) for v in value]\n",
    "                #if multiple area types in value\n",
    "                if \"or\" in value[0]:\n",
    "                    temp = value[0].split(\"or\")\n",
    "                    for v in range(len(temp)):\n",
    "                        if \"km2\" in temp[v] or \"km²\" in temp[v]:\n",
    "                            value = [temp[v].strip(\" \")]\n",
    "                \n",
    "                # add section to key, replace spaces.\n",
    "                if section and section != key:\n",
    "                    key = section + ' ' + key\n",
    "                key = key.replace(' ', '_')\n",
    "                key = key.replace('\\xa0', '_')\n",
    "\n",
    "                # add all this info to data dict.\n",
    "                data[key] = value\n",
    "                if year:\n",
    "                    data[key + '_year'] = year\n",
    "                if value_links: \n",
    "                    data[key + '_links'] = value_links\n",
    "        sect_change = False\n",
    "    if 'area' in data:\n",
    "        if \";\" in data['area'][0]:\n",
    "            if \"km2\" in data['area'][0]:\n",
    "                areas = data['area'][0].split(\";\")\n",
    "                chosen = [zz for zz in areas if 'km2' in zz]\n",
    "                area = chosen[0].replace('\\xa0',' ')\n",
    "                area = area.strip(\" \")\n",
    "                data['area'][0] = area\n",
    "        if \"ha\" in data['area'][0]:\n",
    "            try:\n",
    "                area = float(data['area'][0].split(\" \")[0]) / 100\n",
    "                data['area'][0] = str(area) + \" km2\"\n",
    "            except:\n",
    "                data['area'][0] += \" flag\"\n",
    "        if \"sq mi\" in data['area'][0]:\n",
    "            area = float(data['area'][0].split(\" \")[0]) * 2.58999\n",
    "            data['area'][0] = str(area) + \" km2\"\n",
    "    \n",
    "    \n",
    "    tmp = list(data.keys())\n",
    "    translations = translator.translate(tmp)\n",
    "    \n",
    "    t_data = {}\n",
    "    \n",
    "    for t in translations:\n",
    "        temp = []\n",
    "        for y in data[t.origin]:\n",
    "            try: z = translator.translate(y, src = lan)\n",
    "            except: z = translator.translate(y)\n",
    "            if z.text: temp.append(z.text) \n",
    "        t.text = t.text.replace('com._autónoma', 'autonomous_community')\n",
    "        t_data[t.text] =  temp\n",
    "        \n",
    "    return t_data\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def process_page_2(page):\n",
    "    data = {}\n",
    "    \n",
    "    data['title'] = page.find('h1', {'class': 'firstHeading'}).text\n",
    "    if data['title'].startswith('File:'): return None\n",
    "    ## print(data['title'])\n",
    "    \n",
    "    # get coords in decimal degrees\n",
    "    latlon = page.find('span', {'class': 'geo'})\n",
    "    if latlon:\n",
    "        if lan:\n",
    "            data['lat'] = float(latlon.text.split(\",\")[0].strip())\n",
    "            data['lng'] = float(latlon.text.split(\",\")[1].strip())\n",
    "        else:\n",
    "            if \",\" not in str(latlon.text): \n",
    "                data['lat'] = float(latlon.text.split(\";\")[0])\n",
    "                data['lng'] = float(latlon.text.split(\";\")[1])\n",
    "    \n",
    "    \n",
    "    # attempt to get a wiki_iso. \n",
    "    data['wiki_iso'] = None\n",
    "    infobox = page.find('table', {'class': 'infobox'})\n",
    "    if infobox:\n",
    "        for i in infobox.find_all('a'):\n",
    "            if i.has_attr('title'):\n",
    "                title = i['title']\n",
    "                if title in title_iso:\n",
    "                    data['wiki_iso'] = title_iso[title]\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "    if not data['wiki_iso']:\n",
    "        ps = [p for p in page.find_all('p') if not p.has_attr('class')]\n",
    "        if len(ps) >= 1:\n",
    "            for i in ps[0].find_all('a'):\n",
    "                if i.has_attr('title'):\n",
    "                    title = i['title']\n",
    "                    if title in title_iso:\n",
    "                        data['wiki_iso'] = title_iso[title]\n",
    "                        break\n",
    "    \n",
    "    data['infobox'] = get_infobox_2(page)\n",
    "    \n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        if 'country' in data['infobox']:\n",
    "            if data['infobox']['country'][0] in title_iso:\n",
    "                data['wiki_iso'] = title_iso[data['infobox']['country'][0]]\n",
    "    \n",
    "    # These 3 introduce a lot of noise. Indiana -> India, for example. \n",
    "    # necessary for some actors though. :(\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        ps = [p for p in page.find_all('p') if not p.has_attr('class')]\n",
    "        if len(ps) >= 1:\n",
    "            for k, v in title_iso.items():\n",
    "                if k in ps[0].text:\n",
    "                    data['wiki_iso'] = v\n",
    "                    break\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        if infobox:\n",
    "            for k, v in title_iso.items():\n",
    "                if k in infobox.text:\n",
    "                    data['wiki_iso'] = v\n",
    "                    break\n",
    "    \n",
    "    if not data['wiki_iso']:\n",
    "        for k, v in title_iso.items():\n",
    "            if k in data['title']:\n",
    "                data['wiki_iso'] = v\n",
    "                break\n",
    "    \n",
    "    data['categories'] = []\n",
    "    cat = page.find('div', {'id': 'mw-normal-catlinks'})\n",
    "    if cat:\n",
    "        data['categories'] = [i.text for i in cat.find_all('li')]\n",
    "    if 'category_label' in data['infobox']:\n",
    "        data['categories'].append(data['infobox']['category_label'])  \n",
    "\n",
    "    tmp = []\n",
    "    \n",
    "    for y in data['categories']:\n",
    "        z = translator.translate(y)\n",
    "        tmp.append(z.text)\n",
    "\n",
    "    data['categories'] = tmp    \n",
    "    \n",
    "    data['wiki_type'] = infer_type(data['categories'])\n",
    "    if type(data['wiki_type']) == tuple : data['wiki_type'] = data['wiki_type'][0]\n",
    "    \n",
    "    \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'area' in k or 'surface' in k and 'links' not in k and 'year' not in k and 'code' not in k \\\n",
    "            and 'coordinates' not in k and k != 'areas' and 'rank' not in k:\n",
    "                if not v: continue\n",
    "                data['area'] = data['infobox'][k] \n",
    "                data['area'], data['area_units'] = clean_numbers(data['area'][0])\n",
    "                if k+'_year' in data['infobox']:\n",
    "                    data['area_year'] = data['infobox'][k+'_year']\n",
    "                break  \n",
    "            \n",
    "    data['population'] = []\n",
    "\n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'population' in k and 'links' not in k and 'year' not in k and 'municipality' not in k and v:\n",
    "            if 'rank' not in k and 'density' not in k and 'percent' not in k and 'ethnic' not in k \\\n",
    "                and 'change' not in k and 'demonym' not in k and '_by_' not in k and 'median' not in k \\\n",
    "                and 'household' not in k and v[0] != '' and 'largest' not in k and 'for' not in v[0] \\\n",
    "                and 'language' not in k and not k.endswith('population_') and 'gender' not in k \\\n",
    "                and 'pop_2011–2016' not in k: \n",
    "                    if 'estimate' in k and k+'_year' in data['infobox'].keys():\n",
    "                        yy = data['infobox'][k+'_year']\n",
    "                        data['population'].append([yy+'_'+k,data['infobox'][k]])\n",
    "                    else:\n",
    "                        data['population'].append([k,data['infobox'][k]])\n",
    "                    for pop in data['population']:\n",
    "                        if type(pop[1]) != float and type(pop[1]) != 'NoneType' and pop[1]:\n",
    "                            pop[1], _ = clean_numbers(pop[1][0])\n",
    "                    \n",
    "    if data['population']:\n",
    "        k = data['population'][0][0]\n",
    "        if k+'_year' in data['infobox'].keys():\n",
    "            data['population_year'] = data['infobox'][k+'_year']\n",
    "        for tem in data['population']:\n",
    "            tem[0] = tem[0].replace(',','')\n",
    "            tem[0] = tem[0].replace('population_', '')\n",
    "            tem[0] = tem[0].replace('greater_toronto_area','metro')\n",
    "            tem[0] = tem[0].replace('†_city_proper._', '')\n",
    "            if \"(\" in tem[0]: tem[0] = tem[0].split(\"_\")[-1]\n",
    "            if 'population_year' not in data:\n",
    "                if 'census' in tem[0]:\n",
    "                    data['population_year'] = tem[0].split(\"_\")[0]\n",
    "                elif 'estimate' in tem[0]:\n",
    "                    data['population_year'] = tem[0].split(\"_\")[0]\n",
    "        #data['population'][0][0] = 'population'\n",
    "    else:\n",
    "        del data['population']\n",
    "    \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'gdp' in k and 'links' not in k and 'year' not in k and 'hdi' not in k:\n",
    "            data['gdp'] = data['infobox'][k]\n",
    "            if 'USD' in data['gdp'][0] and 'CNY' in data['gdp'][0]:\n",
    "                data['gdp'] = [\"USD\" + data['gdp'][0].split(\"USD\")[1]]\n",
    "            data['gdp'], data['gdp_units'] = clean_numbers(data['gdp'][0])\n",
    "            if k+'_year' in data['infobox']:\n",
    "                data['gdp_year'] = data['infobox'][k+'_year']\n",
    "            break\n",
    "                \n",
    "    for k,v in data['infobox'].items():\n",
    "        if 'elevation' in k and 'links' not in k and 'year' not in k:\n",
    "            data['elevation'] = data['infobox'][k][0]\n",
    "            break\n",
    "      \n",
    "    # data['title'] = translator.translate(data['title']).text\n",
    "    return data\n",
    "\n",
    "\n",
    " \n",
    "# In[9]:\n",
    "\n",
    "def get_data_2(name, iso, entity_type, check_country=True, debug=False, check_disambig=True):\n",
    "    name = name.replace('!', '')\n",
    "    name = name.replace('Aggregazione ', '')\n",
    "    titles = search_2(name, iso)\n",
    "    if not check_country and debug: print(titles)\n",
    "    pages = get_pages_2(titles, iso)\n",
    "    if not pages:\n",
    "        if debug: print('pages null')\n",
    "        return {}\n",
    "    pages = [process_page_2(page) for page in pages]\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        for p in pages: print(p)\n",
    "\n",
    "    if len(pages) != 0:\n",
    "        disambig = [i for i in pages if i['wiki_type'] == 'Disambiguation']\n",
    "        if len(disambig) > 0: disambig = disambig[0]\n",
    "        else: disambig = None\n",
    "        \n",
    "      \n",
    "        # filter iso\n",
    "        iso_pages = [i for i in pages if 'wiki_iso' in i]\n",
    "        iso_pages = [i for i in iso_pages if i['wiki_iso'] == iso]\n",
    "        \n",
    "        \n",
    "        if len(iso_pages) == 0:\n",
    "            return {}\n",
    "            \n",
    "        # filter entity_type\n",
    "        et_pages = [i for i in iso_pages if i['wiki_type'] == entity_type]\n",
    "     \n",
    "        if len(et_pages) == 0:\n",
    "            return {}\n",
    "        \n",
    "        else:\n",
    "            max_score = 0\n",
    "            rank = 0\n",
    "            for page_num in range(len(et_pages)):\n",
    "                score = infer_type(et_pages[page_num]['categories'])[1]\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    rank = page_num\n",
    "            data = et_pages[rank]\n",
    "            data['region_hierarchy'] = get_region_hierarchy(data)\n",
    "        \n",
    "            if data['infobox'] == {} or data['infobox'] == None:\n",
    "                if debug: print('no infobox')\n",
    "\n",
    "            del data['infobox']\n",
    "            data['categories'] = '; '.join(data['categories'])\n",
    "            \n",
    "            if 'population_year' in data:\n",
    "                data['population_year'] = data['population_year'][0]\n",
    "                    \n",
    "            return data\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Cell\n",
    "\n",
    "get_data_2(\"Abertura\", \"ESP\", \"City\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
